# 17/11/2025 - Day 1
## PRICAI / IVCNZ / AIRAC Conference Notes 

https://pricai.org/2025/

# Tutorial: Neural Network Design and Large Language Models (NASL2M)
## Presenter: Nelishia Pillay (University of Pretoria)

Email: nelishia.pillay@up.ac.za

Keywords: Neural architecture search, large language models

Description: Neural architecture Search (NAS) has proven to be essential for the generation of neural network architectures to solve image classification, segmentation and language translation problems. With the rapid development of the area of large language models (LLMs) a synergistic relationship has developed between NAS and LLMs. NAS has been effective in developing efficient architectures for easier deployment of LLMs while LLMs have been used for NAS. This tutorial examines this synergistic relationship.

The tutorial firstly gives an overview of NAS including the purpose of NAS, approaches used, performance evaluation, including performance estimation using proxies, surrogates and predictors and efficient NAS (ENAS) and NAS benchmarks. The tutorial will then provide an overview of LLMs including descriptions of the different LLMs and related challenges. The use of NAS for the design of LLMs including LLM distillation, LLM compression, hardware-efficient LLMs and fair LLMs will be presented. The tutorial will then look at how LLMs can be used to improve NAS. The topics that will be examined include architecture generation, parameter tuning, knowledge transfer, performance prediction and LLM hybrids.

- Genetic Algorithms (GA) or Large Langauge Models (LLMs) to design neural networks.
- Prompt engineering for optimal Neural Architecutre Search (NAS).
- NAS good for Hyper-parameter Optimization (HPO).
- Multiple objectives (cost + performance + bandwidth).
- Datasets - CIFAR 10, CIFAR 100, Imagenet, NAS Benchmarks.
- Applications - fault detection in hydro turbines. 
- NAS Interpretability - medical diagnosis.
- Other search strategies:
    - Hyper-heuristics 
    - Geneitc programming (GP)
    - Structure-based genetic programming
- Challenges
    - Supervsied learning 
    - Data leakage in using LLMs
    - Heuristic space rather than solutions space 
    - Prompt engineering 
    - Hallucinations 
    - Interpretability 
    - Computational cost (i.e., predictors, surrogate models, pruning, distilling).
- Reasoning models have not been explored for interpretability of LLM NAS. 
- Hyper-heuristic - working in the space that will give you operators to design the neural network
- Structure-based genetic programming 
    - Global level search (exploration) 
    - Local level search (exploitation)
- Unimodal learning - one modality
- Multi-modal machine learning - two or more modalities 

# Workshop 1: 2025 Principle and practice of data and Knowledge Acquisition Workshop
## The PKAW workshop series has been an integral part of PRICAI for nearly two decades.

PKAW has provided a forum for researchers and practitioners to discuss the state-of-the-art in the area of knowledge acquisition and machine intelligence (MI, also Artificial Intelligence, AI). PKAW 2025 will continue the above focus and welcome the contributions to the multi-disciplinary approach of human and big data-driven knowledge acquisition and AI techniques and applications.

Organizers:

Dr. Shiqing Wu, City University of Macau, Macao SAR (sqwu@cityu.edu.mo)
Dr. Weihua Li, Auckland University of Technology, New Zealand (weihua.li@aut.ac.nz)

Website:

http://pkawwebsite.github.io/2025

# Data Profile Generation Framework for Data Utilization (Authors: Issei 
## Matsumoto, Tomokazu Matsui, Yukihisa Fujita, Hirohiko Suwa, and 
Keiichi Yasumoto

- Automatic evaluation:
    - Consistency
    - Fluency 
    - Coherance 
    - Relevance 
- Reference texts for dataset profiles no not exist in the real world.
- Text generated by LLMs rarely contain grammatical errors. 
- Proposed a framework that enables non-specialists to understand how to utilize data and identify possible use cases. 

# OfficeMind: Dynamic Agent Collaboration in Multi-Agent Systems for 
Office Resource Management 
## (Authors: Guo Yiming and Raymond S. T. Lee

- Develop a novel MAS framework for dynamic resource coordination.
- Solves gaps: domain misalignment, negotation, rigidity, integration barriers. 
- Evaluation in real world 200 people office. 
- Limitations:
    - Static allocation 
    - Office-specific mechanisms
    - Integration barriers
- Seemless integration with Outlook / Slack. 
- Multi-agent System

# STMMoE: A Spatio-Temporal Multimodal Mixture-of-Experts Model 
for Urban Traffic Prediction 
## (Authors: Kenan Kang, Matthew M.Y. Kuo, and Weihua Li)

- Heterogenous data modalitites.
- Real-world multimodal traffic dataset (London, UK).
- Generalization accross diverse urban subregions. 
- Input data: 
    - Traffic flow data 
    - Weather conditions 
    - Event data 
    - Calendar attributes 

# ClinAug: Closing the Performance Gap in Medical Error Detection with 
Low-Cost Augmentation and ClinicalBERT 
## (Authors: Junli Dai, Yuming Li, and Farhaan Mirza)

# Tutorial 3: Adaptive Machine Learning
## Presenters: Heitor Murilo Gomes (Victoria University of Wellington), Anton Lee (Victoria University of Wellington), Yibin Sun (University of Waikato)

Email: heitor.gomes@vuw.ac.nz, anton.lee@ecs.vuw.ac.nz, yibin.spencer.sun@gmail.com

Keywords: Artificial Intelligence, Machine Learning, Data Streams, Online Continual Learning, Concept Drifts

Description: Adaptive Machine Learning (AML) is a hands-on tutorial that introduces real-time, incremental learning techniques for streaming and continually evolving data. Using CapyMOA, an open-source Python library, participants will explore practical tools and algorithms that adapt to changing data distributions, enabling robust, low-latency learning in dynamic environments. Ideal for researchers and practitioners aiming to build scalable, adaptive solutions.

https://github.com/adaptive-machine-learning/PRICAI2025/tree/main

# Overview
## Heitor Gomes

- Data streams: sequence of items, possibly infinite, each item having a timestamp, and so a temporal order. 
- Examples: 
    - Sensor data (IoT)
    - Marketing and e-commerce 
    - Cybersecurity 
- When to stream learning:
    - Can't store all the data (i.e., volume or velocity).
    - Shouldn't store all the data. (i.e., privacy concerns, compliance requirements).
- most recent -> most relevant -> discard or aggregrate 
- Streaming data:
    - Continious flow of data.
    - Limited time to inspect data points. 
    - Interleaved phases.
- Challenges:
    - concept drifts
    - concept evolution 
- Batch is different from streaming. 
- The learning cycle requirements: 
    1. Process an example at a time 
    2. USe a limited amount of memory 
    3. Work in a limited amount of time.
    4. Be ready to predict at any point.
- Evaluation metrics 
    - 
- Evaluation framework
    - Cumulative (test-then-train) - average over all instances.
    - Windowed (prequential) - the metrics over a window.
- CapyMOA https://github.com/adaptive-machine-learning/CapyMOA 
- Evolving Steam Learning 
    - The world is dynamic... changes occur all the time.
    - These changes affect our machine learning models.
- Ideally:
    1. Detect, understand and react to changes.
    2. Learn new concepts without forgetting old concepts. 
- Assumptions:
    - Indepedent and identically distributed (iid)
- Concept drift - change in distribution over time. 
- Rate of change:
    - sudden
    - incremental
    - gradual
    - reoccuring concepts 
    - outlier 
- ADapative WINdow (ADWIN) - concept drift algorithm
    - Statistical algorithm doing the test between the two distribution.
    - Doesn't store all the data, stores a compressed window.
    - Adapative windows of variable size. 

# Supervised Learning 
## Yibin Sun

- Classification and regression
- Classification algorithms:
    - Decision Tree Classifier (i.e., C4.5, CART, ID3)
        - Hoeffding Tree 
        - Extremely Fast Decision Tree (EFDT) 
        - PLASTIC - extension of EFDT
    - Ensemble Learners
        - Boosting - trains many models sequentially.
        - Bagging - trains many independent models in parallel.
    - Adaptive Random Forest (ARF)
        - Streaming version of the original Random Froest 
        - Uses a variation of the Hoeffding Tree. 
    - Other streaming ensemble methods 
        - Dynamic Weighted Majority 
        - Leveraging Bagging 
- Boosting on streams 
    - OzaBoost 
    - Online Smooth Boost 
    - Gradeint boosted AXGB 
    - Streaming Gradient Boosted Tree (SGBT)
- Regression algorithms:
    - FIRTDD
    - Self-Optimizing k-Nearest Leaves (SOKNL)

# Online Continual Learning 
## Anton Lee 

- Combines online learning and continual learning.
- Continual learning focuses on developing artificial neural networks that can learn new concepts and changing concepts. 
- Life-long learning, synonym.
- Tasks:
    - Class incremental - learn to classify new classes. 
    - Domain incremental - update model to accomadate changes within existing classes.
    - Graceful/selecting forgetting - forget what is no longer needed. 
- Catastrophic forgetting - ANNs tend to forget previously learned tasks when trained on a new sequence of tasks. 
- Catastrophic - is used for historic reasons to convery a severity not present in biological forgetting.
- Why? It happens because of distrubuted representations. Any new learning impacts all network parameters to some degreee, overwriting previous knowledge. 
-  Plasticity: The ability to learn new things 
- Stability: the ability to remember old things 
- This trade-off is a recurring theme in continual learning. 
- Forward transfer - forward transfer occurs when a model learnign a task imrpvoed perfomance on, or the ability to learn futur tasks.
- Backwards transfer: backwards transfer occurs whena model learning task improved performance on past tasks. 
- Status quo: deep learning models (artificial neural nets) learn by sampling from a single training set that contains all tasks.
- Continual learning tackles catastrophic forgetting in artificial neural networks. 
- Online continual learnong focuses on developing ANNs tat can pefrm inference at any time, learn from datastreams, and adapt to new and changing tasks without forgetiing.
- Scenarios:
    - Task incremental learning 
    - Class incremental learning 
    - Domain incremental learning 
    - Online class incremental 
    - Online domain incremental 
- The Stability Gap - a temporary drop in performance on past tasks after a transitiion to new tasks. 
- Taxonomy: Regularization, Replay, Prototype, Architecture 
- Replay -> Stops forgetting by storing blocks from the past.
    - Exerience replay -> resoervoir sampling to uniformly sample from a bugger of past examples.
    - GDumb -> is a greedy sampler combined with a "dumb" learner (not Continual Learning).
    - Repeated Augmented Rehearsal (RAR) -> random augmentations on repeated examples.
    - Averaged Gradient Episodoc Memory (A-GEM) -> constrain optimization using gradient projection.
    - Gradient-Based Sample Selection (GSS) -> maximize the diversity of gradient directions.
    - Maximally Interfered Retrieval (MIR) -> top-k samples from a replay buffer
- Regularization -> encourages the model to remain similar to a previous model 
    - (1) weight regularization, (2) functional regularization 
    - Weight regularizaiton -> mimic the weights of the older model. 
    - Functional Regularisation (LWF) -> mimic the behaviour of the older model.
- Prototypes -> Backbone (e.g., ViT) and Head (e.g., KNN)
    - Nearest Class Mean (NCM) -> mean of each class in embedding space.
    - Streaming Linear Discriminant Analysis (SLDA).
    - Incremental Classifier and Representation Learning (iCaRL) -> 
    - Supvervised Contrastive Replay (SCR) -> pushed similar instances together, dissimilar instances apart from eachother. 
    - RanPAC and Pre-Processing - uses random fourier features to improve the linear separability of features.
- Architecture 
    - Fixed Network Parameter Isolation -> Freeze parts of the network that have learned a task, and learn it forever. No positive backwards transfer, can benefit from positive forwards transfer. 
    - Dynamic Architecture Parameter Isolation -> what if we can grow the network?
    - Ensemble -> One network for each task - with a gate.  
    - Modular -> recombine modules from previous tasks.
    - Parameter Efficient Fine-tuning - weights, and modules, adapters.

