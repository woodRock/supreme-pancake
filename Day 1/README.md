# 17/11/2025 - Day 1
## PRICAI / IVCNZ / AIRAC Conference Notes 

https://pricai.org/2025/

# Tutorial: Neural Network Design and Large Language Models (NASL2M)
## Presenter: Nelishia Pillay (University of Pretoria)

Email: nelishia.pillay@up.ac.za

Keywords: Neural architecture search, large language models

Description: Neural architecture Search (NAS) has proven to be essential for the generation of neural network architectures to solve image classification, segmentation and language translation problems. With the rapid development of the area of large language models (LLMs) a synergistic relationship has developed between NAS and LLMs. NAS has been effective in developing efficient architectures for easier deployment of LLMs while LLMs have been used for NAS. This tutorial examines this synergistic relationship.

The tutorial firstly gives an overview of NAS including the purpose of NAS, approaches used, performance evaluation, including performance estimation using proxies, surrogates and predictors and efficient NAS (ENAS) and NAS benchmarks. The tutorial will then provide an overview of LLMs including descriptions of the different LLMs and related challenges. The use of NAS for the design of LLMs including LLM distillation, LLM compression, hardware-efficient LLMs and fair LLMs will be presented. The tutorial will then look at how LLMs can be used to improve NAS. The topics that will be examined include architecture generation, parameter tuning, knowledge transfer, performance prediction and LLM hybrids.

- Genetic Algorithms (GA) or Large Langauge Models (LLMs) to design neural networks.
- Prompt engineering for optimal Neural Architecutre Search (NAS).
- NAS good for Hyper-parameter Optimization (HPO).
- Multiple objectives (cost + performance + bandwidth).
- Datasets - CIFAR 10, CIFAR 100, Imagenet, NAS Benchmarks.
- Applications - fault detection in hydro turbines. 
- NAS Interpretability - medical diagnosis.
- Other search strategies:
    - Hyper-heuristics 
    - Geneitc programming (GP)
    - Structure-based genetic programming
- Challenges
    - Supervsied learning 
    - Data leakage in using LLMs
    - Heuristic space rather than solutions space 
    - Prompt engineering 
    - Hallucinations 
    - Interpretability 
    - Computational cost (i.e., predictors, surrogate models, pruning, distilling).
- Reasoning models have not been explored for interpretability of LLM NAS. 
- Hyper-heuristic - working in the space that will give you operators to design the neural network
- Structure-based genetic programming 
    - Global level search (exploration) 
    - Local level search (exploitation)
- Unimodal learning - one modality
- Multi-modal machine learning - two or more modalities 

# Workshop 1: 2025 Principle and practice of data and Knowledge Acquisition Workshop
## The PKAW workshop series has been an integral part of PRICAI for nearly two decades.

PKAW has provided a forum for researchers and practitioners to discuss the state-of-the-art in the area of knowledge acquisition and machine intelligence (MI, also Artificial Intelligence, AI). PKAW 2025 will continue the above focus and welcome the contributions to the multi-disciplinary approach of human and big data-driven knowledge acquisition and AI techniques and applications.

Organizers:

Dr. Shiqing Wu, City University of Macau, Macao SAR (sqwu@cityu.edu.mo)
Dr. Weihua Li, Auckland University of Technology, New Zealand (weihua.li@aut.ac.nz)

Website:

http://pkawwebsite.github.io/2025

## Talk 1 

- Automatic evaluation:
    - Consistency
    - Fluency 
    - Coherance 
    - Relevance 
- Reference texts for dataset profiles no not exist in the real world.
- Text generated by LLMs rarely contain grammatical errors. 
- Proposed a framework that enables non-specialists to understand how to utilize data and identify possible use cases. 

## 
